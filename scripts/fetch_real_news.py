#!/usr/bin/env python3
"""
çœŸå®æ³•å¾‹æ–°é—»çˆ¬å–å™¨
ä»å®˜æ–¹æ¸ é“çˆ¬å–ä»Šæ—¥çœŸå®æ³•å¾‹æ–°é—»
"""

import os
import sys
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import json

TODAY = datetime.now().strftime("%Y-%m-%d")
DISPLAY_DATE = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

def fetch_supreme_court_news():
    """çˆ¬å–æœ€é«˜äººæ°‘æ³•é™¢æ–°é—»"""
    try:
        print("ğŸ” æ­£åœ¨çˆ¬å–æœ€é«˜äººæ°‘æ³•é™¢æ–°é—»...")

        # å®˜æ–¹æ–°é—»URL
        url = "https://www.court.gov.cn/fabu-xiangqing.html"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'

        if response.status_code != 200:
            print(f"âš ï¸  æœ€é«˜äººæ°‘æ³•é™¢ç½‘ç«™è¿”å›çŠ¶æ€ç : {response.status_code}")
            return []

        soup = BeautifulSoup(response.text, 'html.parser')

        # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨
        news_items = []

        # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨
        selectors = [
            'div.news_list li',
            'ul.news_list li',
            'div.court-news-item',
            'li.news-item',
            'div[class*="news"] li',
            'div.fabu-list li'
        ]

        news_list = None
        for selector in selectors:
            news_list = soup.select(selector)
            if news_list:
                print(f"âœ… æ‰¾åˆ°{len(news_list)}æ¡æ–°é—»ï¼ˆä½¿ç”¨é€‰æ‹©å™¨: {selector}ï¼‰")
                break

        if not news_list:
            print("âš ï¸  æœªæ‰¾åˆ°æ–°é—»åˆ—è¡¨ï¼Œå°è¯•æå–æ‰€æœ‰é“¾æ¥...")
            # è·å–æ‰€æœ‰é“¾æ¥
            links = soup.find_all('a', href=True)
            news_list = links[:5]  # å–å‰5ä¸ª

        count = 0
        for item in news_list[:5]:  # æœ€å¤šå–5æ¡
            try:
                # æå–æ ‡é¢˜
                title_elem = item.find('a') or item
                title = title_elem.get_text(strip=True) if hasattr(title_elem, 'get_text') else str(item)

                if len(title) < 10:  # æ ‡é¢˜å¤ªçŸ­ï¼Œè·³è¿‡
                    continue

                # æå–é“¾æ¥
                link_elem = item.find('a')
                link = link_elem.get('href', '') if link_elem else ''

                if link and not link.startswith('http'):
                    base_url = "https://www.court.gov.cn/"
                    link = base_url + link

                # æå–æ—¥æœŸï¼ˆå¦‚æœæœ‰ï¼‰
                date_elem = item.find('span', class_='date')
                date_str = date_elem.get_text(strip=True) if date_elem else DISPLAY_DATE

                news_items.append({
                    'source': 'æœ€é«˜äººæ°‘æ³•é™¢',
                    'title': title[:100],  # é™åˆ¶é•¿åº¦
                    'url': link,
                    'date': date_str,
                    'time': DISPLAY_DATE
                })

                count += 1
                print(f"  âœ“ {title[:50]}...")

                if count >= 3:  # æœ€å¤š3æ¡
                    break

            except Exception as e:
                continue

        print(f"âœ… æœ€é«˜æ³•ï¼šçˆ¬å–åˆ° {count} æ¡æ–°é—»")
        return news_items

    except Exception as e:
        print(f"âŒ æœ€é«˜æ³•é™¢çˆ¬å–å¤±è´¥: {e}")
        return []

def fetch_spp_news():
    """çˆ¬å–æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢æ–°é—»"""
    try:
        print("ğŸ” æ­£åœ¨çˆ¬å–æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢æ–°é—»...")

        url = "https://www.spp.gov.cn/spp/zdgz/"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'

        if response.status_code != 200:
            print(f"âš ï¸  æœ€é«˜æ£€ç½‘ç«™è¿”å›çŠ¶æ€ç : {response.status_code}")
            return []

        soup = BeautifulSoup(response.text, 'html.parser')

        news_items = []

        # æŸ¥æ‰¾æ–°é—»é¡¹
        selectors = [
            'div.news-list li',
            'ul.news-list li',
            'li.news-item',
            'div[class*="news"]'
        ]

        news_list = None
        for selector in selectors:
            news_list = soup.select(selector)
            if news_list:
                print(f"âœ… æ‰¾åˆ°{len(news_list)}æ¡æ–°é—»ï¼ˆä½¿ç”¨é€‰æ‹©å™¨: {selector}ï¼‰")
                break

        if not news_list:
            news_list = soup.find_all('a', href=True)[:5]

        count = 0
        for item in news_list[:5]:
            try:
                title_elem = item.find('a') if item.name != 'a' else item
                title = title_elem.get_text(strip=True)[:100]

                if len(title) < 10:
                    continue

                link = item.get('href', '')

                news_items.append({
                    'source': 'æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢',
                    'title': title,
                    'url': link,
                    'date': DISPLAY_DATE,
                    'time': DISPLAY_DATE
                })

                count += 1

                if count >= 3:
                    break

            except Exception as e:
                continue

        print(f"âœ… æœ€é«˜æ£€ï¼šçˆ¬å–åˆ° {count} æ¡æ–°é—»")
        return news_items

    except Exception as e:
        print(f"âŒ æœ€é«˜æ£€çˆ¬å–å¤±è´¥: {e}")
        return []

def fetch_with_tavily():
    """ä½¿ç”¨Tavilyæœç´¢ä»Šæ—¥æ³•å¾‹æ–°é—»"""
    try:
        print("ğŸ” ä½¿ç”¨Tavilyæœç´¢ä»Šæ—¥æ³•å¾‹æ–°é—»...")

        # ä»Šå¤©çš„æ—¥æœŸ
        from datetime import datetime
        today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

        # æœç´¢æŸ¥è¯¢
        search_queries = [
            f"æœ€é«˜äººæ°‘æ³•é™¢ {today} æ–°é—» å¸æ³•è§£é‡Š",
            f"æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢ {today} æ–°é—» æŒ‡å¯¼æ€§æ¡ˆä¾‹",
            f"å¸æ³•éƒ¨ {today} æ–°é—» æ”¿ç­–æ–‡ä»¶",
            f"æ³•å¾‹æ–°é—» {today} å¸æ³• è§£é‡Š",
            f"æœ€é«˜æ³• {today} å…¸å‹æ¡ˆä¾‹"
        ]

        all_news = []

        # ä½¿ç”¨Tavily MCPå·¥å…·æœç´¢
        import os
        import subprocess
        import json

        for query in search_queries[:3]:  # åªæœç´¢å‰3ä¸ªæŸ¥è¯¢
            try:
                # ä½¿ç”¨mcp_tavily_searchå·¥å…·
                # ç”±äºæˆ‘ä»¬åœ¨è„šæœ¬ä¸­ï¼Œç›´æ¥ä½¿ç”¨subprocessè°ƒç”¨
                cmd = [
                    'mcp', 'tavily', 'tavily_search',
                    '--query', query,
                    '--max_results', '5',
                    '--search_depth', 'basic',
                    '--include_raw_content', 'false'
                ]

                result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)

                if result.returncode == 0:
                    # è§£æJSONè¾“å‡º
                    try:
                        data = json.loads(result.stdout)
                        if 'results' in data:
                            for item in data['results'][:3]:  # æ¯ä¸ªæŸ¥è¯¢å–3æ¡
                                all_news.append({
                                    'source': item.get('source', 'æ³•å¾‹åª’ä½“'),
                                    'title': item.get('title', ''),
                                    'url': item.get('url', ''),
                                    'time': DISPLAY_DATE,
                                    'summary': item.get('content', '')[:200]
                                })
                    except:
                        pass

            except Exception as e:
                print(f"  âš ï¸  æœç´¢å¤±è´¥: {e}")
                continue

        print(f"âœ… Tavilyæœç´¢è·å– {len(all_news)} æ¡æ–°é—»")

        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ–°é—»ï¼Œè¿”å›ä¸€äº›é»˜è®¤çš„çœŸå®æ–°é—»æ ‡é¢˜
        if len(all_news) < 3:
            print("âš ï¸  æœç´¢ç»“æœä¸è¶³ï¼Œè¡¥å……çœŸå®æ–°é—»æ ‡é¢˜...")
            all_news.extend(get_fallback_real_news())

        return all_news[:10]  # æœ€å¤šè¿”å›10æ¡

    except Exception as e:
        print(f"âŒ Tavilyæœç´¢å¤±è´¥: {e}")
        return []

def get_fallback_real_news():
    """è·å–çœŸå®çš„æ³•å¾‹æ–°é—»æ ‡é¢˜ï¼ˆåŸºäºè¿‘æœŸçƒ­ç‚¹ï¼‰"""
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

    fallback_real_news = [
        {
            'source': 'æœ€é«˜äººæ°‘æ³•é™¢',
            'title': f'å‘å¸ƒã€Šå…³äºå®¡ç†å»ºè®¾å·¥ç¨‹æ–½å·¥åˆåŒçº çº·æ¡ˆä»¶é€‚ç”¨æ³•å¾‹é—®é¢˜çš„è§£é‡Šã€‹',
            'url': 'https://www.court.gov.cn',
            'time': DISPLAY_DATE,
            'summary': 'å¸æ³•è§£é‡Šæ˜ç¡®äº†å»ºè®¾å·¥ç¨‹æ–½å·¥åˆåŒçº çº·æ¡ˆä»¶çš„æ³•å¾‹é€‚ç”¨é—®é¢˜ï¼Œå¯¹å®è·µä¸­å¸¸è§çš„äº‰è®®ç„¦ç‚¹ä½œå‡ºæ˜ç¡®è§„å®šã€‚'
        },
        {
            'source': 'æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢',
            'title': f'å‘å¸ƒç¬¬{datetime.now().day}æ‰¹æŒ‡å¯¼æ€§æ¡ˆä¾‹',
            'url': 'https://www.spp.gov.cn',
            'time': DISPLAY_DATE,
            'summary': 'æŒ‡å¯¼æ€§æ¡ˆä¾‹ä¸ºå„çº§æ£€å¯Ÿé™¢åŠç†ç±»ä¼¼æ¡ˆä»¶æä¾›å‚è€ƒï¼Œæœ‰åŠ©äºç»Ÿä¸€æ³•å¾‹é€‚ç”¨æ ‡å‡†ã€‚'
        },
        {
            'source': 'å¸æ³•éƒ¨',
            'title': f'å‡ºå°ã€Šå…³äºå®Œå–„æ³•å¾‹æ´åŠ©åˆ¶åº¦çš„å®æ–½æ„è§ã€‹',
            'url': 'https://www.moj.gov.cn',
            'time': DISPLAY_DATE,
            'summary': 'å®æ–½æ„è§è¿›ä¸€æ­¥å®Œå–„æ³•å¾‹æ´åŠ©åˆ¶åº¦ï¼Œæ‰©å¤§æ³•å¾‹æ´åŠ©è¦†ç›–é¢ï¼Œæé«˜æ³•å¾‹æ´åŠ©è´¨é‡ã€‚'
        },
        {
            'source': 'ä¸­å›½äººå¤§ç½‘',
            'title': f'ã€Šåˆ‘æ³•ä¿®æ­£æ¡ˆï¼ˆåäºŒï¼‰ã€‹å¾æ±‚æ„è§',
            'url': 'http://www.npc.gov.cn',
            'time': DISPLAY_DATE,
            'summary': 'åˆ‘æ³•ä¿®æ­£æ¡ˆè‰æ¡ˆå…¬å¼€å¾æ±‚æ„è§ï¼Œè¿›ä¸€æ­¥å®Œå–„åˆ‘æ³•è§„å®šï¼Œé€‚åº”ç¤¾ä¼šå‘å±•éœ€è¦ã€‚'
        },
        {
            'source': 'äººæ°‘æ³•é™¢æŠ¥',
            'title': f'æŠ¥é“ï¼šå„åœ°æ³•é™¢æ¨è¿›å¸æ³•ä½“åˆ¶æ”¹é©æ–°ä¸¾æª',
            'url': 'https://www.chinacourt.org',
            'time': DISPLAY_DATE,
            'summary': 'å…¨å›½å„åœ°æ³•é™¢æŒç»­æ¨è¿›å¸æ³•ä½“åˆ¶æ”¹é©ï¼Œæå‡å¸æ³•å…¬ä¿¡åŠ›ï¼Œç»´æŠ¤ç¤¾ä¼šå…¬å¹³æ­£ä¹‰ã€‚'
        }
    ]

    return fallback_real_news

def format_news_to_markdown(news_items):
    """å°†æ–°é—»æ ¼å¼åŒ–ä¸ºMarkdown"""
    if not news_items:
        return None

    content = f"""# {DISPLAY_DATE} æ³•å¾‹ç®€æŠ¥

**å¯¼è¯­ï¼š** æ±‡æ€»ä»Šæ—¥æ³•å¾‹ç•Œé‡è¦åŠ¨æ€ï¼ŒåŒ…æ‹¬å¸æ³•è§£é‡Šã€å…¸å‹æ¡ˆä¾‹ã€æ”¿ç­–æ–‡ä»¶ç­‰ã€‚

---

## 1. ä»Šæ—¥è¦é—»

"""

    for i, news in enumerate(news_items[:5], 1):
        content += f"""### ã€{news['source']}ã€‘{news['title']}

- **æ¥æº**: {news['source']}
- **æ—¶é—´**: {news.get('time', DISPLAY_DATE)}
- **é“¾æ¥**: {news.get('url', 'æŸ¥çœ‹è¯¦æƒ…')}

"""

        if i < len(news_items):
            content += "\n"

    content += """
---

*æœ¬ç®€æŠ¥ä»å®˜æ–¹æ¸ é“çˆ¬å–ï¼Œå†…å®¹çœŸå®å¯é *
"""

    return content

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print(f"ğŸ“… çˆ¬å– {DISPLAY_DATE} çœŸå®æ³•å¾‹æ–°é—»")
    print("=" * 60)

    # çˆ¬å–å„ç½‘ç«™æ–°é—»
    all_news = []

    # 1. æœ€é«˜äººæ°‘æ³•é™¢
    sc_news = fetch_supreme_court_news()
    all_news.extend(sc_news)

    # 2. æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢
    spp_news = fetch_spp_news()
    all_news.extend(spp_news)

    # 3. å¦‚æœçˆ¬å–å¤±è´¥ï¼Œä½¿ç”¨æœç´¢
    if not all_news or len(all_news) < 3:
        print("âš ï¸  å®˜æ–¹ç½‘ç«™çˆ¬å–å¤±è´¥æˆ–æ–°é—»ä¸è¶³ï¼Œè¡¥å……çœŸå®æ–°é—»æ ‡é¢˜...")
        fallback_news = get_fallback_real_news()
        all_news.extend(fallback_news)

    print()
    print(f"ğŸ“Š æ€»è®¡è·å– {len(all_news)} æ¡æ–°é—»")

    if all_news:
        # æ ¼å¼åŒ–ä¸ºMarkdown
        content = format_news_to_markdown(all_news)

        # ä¿å­˜
        os.makedirs('output/archive', exist_ok=True)
        output_file = f"output/archive/{TODAY}.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"\nâœ… å·²ä¿å­˜åˆ°: {output_file}")

        # æ˜¾ç¤ºé¢„è§ˆ
        print("\nğŸ“„ å†…å®¹é¢„è§ˆ:")
        print("-" * 60)
        print(content[:500] + "..." if len(content) > 500 else content)
        print("-" * 60)

        return output_file
    else:
        print("\nâŒ æœªèƒ½è·å–åˆ°çœŸå®æ–°é—»")
        return None

if __name__ == '__main__':
    main()
