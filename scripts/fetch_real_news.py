#!/usr/bin/env python3
"""
çœŸå®æ³•å¾‹æ–°é—»çˆ¬å–å™¨
ä»å®˜æ–¹æ¸ é“çˆ¬å–ä»Šæ—¥çœŸå®æ³•å¾‹æ–°é—»
"""

import os
import sys
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import json

TODAY = datetime.now().strftime("%Y-%m-%d")
DISPLAY_DATE = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

def fetch_supreme_court_news():
    """çˆ¬å–æœ€é«˜äººæ°‘æ³•é™¢æ–°é—»"""
    try:
        print("ğŸ” æ­£åœ¨çˆ¬å–æœ€é«˜äººæ°‘æ³•é™¢æ–°é—»...")

        # å®˜æ–¹æ–°é—»URL
        url = "https://www.court.gov.cn/fabu-xiangqing.html"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'

        if response.status_code != 200:
            print(f"âš ï¸  æœ€é«˜äººæ°‘æ³•é™¢ç½‘ç«™è¿”å›çŠ¶æ€ç : {response.status_code}")
            return []

        soup = BeautifulSoup(response.text, 'html.parser')

        # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨
        news_items = []

        # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨
        selectors = [
            'div.news_list li',
            'ul.news_list li',
            'div.court-news-item',
            'li.news-item',
            'div[class*="news"] li',
            'div.fabu-list li'
        ]

        news_list = None
        for selector in selectors:
            news_list = soup.select(selector)
            if news_list:
                print(f"âœ… æ‰¾åˆ°{len(news_list)}æ¡æ–°é—»ï¼ˆä½¿ç”¨é€‰æ‹©å™¨: {selector}ï¼‰")
                break

        if not news_list:
            print("âš ï¸  æœªæ‰¾åˆ°æ–°é—»åˆ—è¡¨ï¼Œå°è¯•æå–æ‰€æœ‰é“¾æ¥...")
            # è·å–æ‰€æœ‰é“¾æ¥
            links = soup.find_all('a', href=True)
            news_list = links[:5]  # å–å‰5ä¸ª

        count = 0
        for item in news_list[:5]:  # æœ€å¤šå–5æ¡
            try:
                # æå–æ ‡é¢˜
                title_elem = item.find('a') or item
                title = title_elem.get_text(strip=True) if hasattr(title_elem, 'get_text') else str(item)

                if len(title) < 10:  # æ ‡é¢˜å¤ªçŸ­ï¼Œè·³è¿‡
                    continue

                # æå–é“¾æ¥
                link_elem = item.find('a')
                link = link_elem.get('href', '') if link_elem else ''

                if link and not link.startswith('http'):
                    base_url = "https://www.court.gov.cn/"
                    link = base_url + link

                # æå–æ—¥æœŸï¼ˆå¦‚æœæœ‰ï¼‰
                date_elem = item.find('span', class_='date')
                date_str = date_elem.get_text(strip=True) if date_elem else DISPLAY_DATE

                news_items.append({
                    'source': 'æœ€é«˜äººæ°‘æ³•é™¢',
                    'title': title[:100],  # é™åˆ¶é•¿åº¦
                    'url': link,
                    'date': date_str,
                    'time': DISPLAY_DATE
                })

                count += 1
                print(f"  âœ“ {title[:50]}...")

                if count >= 3:  # æœ€å¤š3æ¡
                    break

            except Exception as e:
                continue

        print(f"âœ… æœ€é«˜æ³•ï¼šçˆ¬å–åˆ° {count} æ¡æ–°é—»")
        return news_items

    except Exception as e:
        print(f"âŒ æœ€é«˜æ³•é™¢çˆ¬å–å¤±è´¥: {e}")
        return []

def fetch_spp_news():
    """çˆ¬å–æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢æ–°é—»"""
    try:
        print("ğŸ” æ­£åœ¨çˆ¬å–æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢æ–°é—»...")

        url = "https://www.spp.gov.cn/spp/zdgz/"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'

        if response.status_code != 200:
            print(f"âš ï¸  æœ€é«˜æ£€ç½‘ç«™è¿”å›çŠ¶æ€ç : {response.status_code}")
            return []

        soup = BeautifulSoup(response.text, 'html.parser')

        news_items = []

        # æŸ¥æ‰¾æ–°é—»é¡¹
        selectors = [
            'div.news-list li',
            'ul.news-list li',
            'li.news-item',
            'div[class*="news"]'
        ]

        news_list = None
        for selector in selectors:
            news_list = soup.select(selector)
            if news_list:
                print(f"âœ… æ‰¾åˆ°{len(news_list)}æ¡æ–°é—»ï¼ˆä½¿ç”¨é€‰æ‹©å™¨: {selector}ï¼‰")
                break

        if not news_list:
            news_list = soup.find_all('a', href=True)[:5]

        count = 0
        for item in news_list[:5]:
            try:
                title_elem = item.find('a') if item.name != 'a' else item
                title = title_elem.get_text(strip=True)[:100]

                if len(title) < 10:
                    continue

                link = item.get('href', '')

                news_items.append({
                    'source': 'æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢',
                    'title': title,
                    'url': link,
                    'date': DISPLAY_DATE,
                    'time': DISPLAY_DATE
                })

                count += 1

                if count >= 3:
                    break

            except Exception as e:
                continue

        print(f"âœ… æœ€é«˜æ£€ï¼šçˆ¬å–åˆ° {count} æ¡æ–°é—»")
        return news_items

    except Exception as e:
        print(f"âŒ æœ€é«˜æ£€çˆ¬å–å¤±è´¥: {e}")
        return []

def fetch_with_tavily():
    """ä½¿ç”¨Tavilyæœç´¢ä»Šæ—¥æ³•å¾‹æ–°é—»"""
    try:
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨tavily MCPå·¥å…·
        print("ğŸ” ä½¿ç”¨Tavilyæœç´¢ä»Šæ—¥æ³•å¾‹æ–°é—»...")

        # æœç´¢ä»Šæ—¥æ³•å¾‹æ–°é—»
        query = f"æ³•å¾‹ {TODAY} æœ€é«˜äººæ°‘æ³•é™¢ æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢"

        # ç”±äºæˆ‘ä»¬åœ¨è„šæœ¬ä¸­ï¼Œæš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
        # å®é™…ä½¿ç”¨æ—¶å¯ä»¥é›†æˆTavily API
        print("âš ï¸  Tavilyæœç´¢éœ€è¦APIå¯†é’¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")

        return []

    except Exception as e:
        print(f"âŒ Tavilyæœç´¢å¤±è´¥: {e}")
        return []

def format_news_to_markdown(news_items):
    """å°†æ–°é—»æ ¼å¼åŒ–ä¸ºMarkdown"""
    if not news_items:
        return None

    content = f"""# {DISPLAY_DATE} æ³•å¾‹ç®€æŠ¥

**å¯¼è¯­ï¼š** æ±‡æ€»ä»Šæ—¥æ³•å¾‹ç•Œé‡è¦åŠ¨æ€ï¼ŒåŒ…æ‹¬å¸æ³•è§£é‡Šã€å…¸å‹æ¡ˆä¾‹ã€æ”¿ç­–æ–‡ä»¶ç­‰ã€‚

---

## 1. ä»Šæ—¥è¦é—»

"""

    for i, news in enumerate(news_items[:5], 1):
        content += f"""### ã€{news['source']}ã€‘{news['title']}

- **æ¥æº**: {news['source']}
- **æ—¶é—´**: {news.get('time', DISPLAY_DATE)}
- **é“¾æ¥**: {news.get('url', 'æŸ¥çœ‹è¯¦æƒ…')}

"""

        if i < len(news_items):
            content += "\n"

    content += """
---

*æœ¬ç®€æŠ¥ä»å®˜æ–¹æ¸ é“çˆ¬å–ï¼Œå†…å®¹çœŸå®å¯é *
"""

    return content

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print(f"ğŸ“… çˆ¬å– {DISPLAY_DATE} çœŸå®æ³•å¾‹æ–°é—»")
    print("=" * 60)

    # çˆ¬å–å„ç½‘ç«™æ–°é—»
    all_news = []

    # 1. æœ€é«˜äººæ°‘æ³•é™¢
    sc_news = fetch_supreme_court_news()
    all_news.extend(sc_news)

    # 2. æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢
    spp_news = fetch_spp_news()
    all_news.extend(spp_news)

    # 3. å¦‚æœçˆ¬å–å¤±è´¥ï¼Œä½¿ç”¨æœç´¢
    if not all_news:
        print("âš ï¸  å®˜æ–¹ç½‘ç«™çˆ¬å–å¤±è´¥ï¼Œå°è¯•æœç´¢...")
        search_news = fetch_with_tavily()
        all_news.extend(search_news)

    print()
    print(f"ğŸ“Š æ€»è®¡è·å– {len(all_news)} æ¡æ–°é—»")

    if all_news:
        # æ ¼å¼åŒ–ä¸ºMarkdown
        content = format_news_to_markdown(all_news)

        # ä¿å­˜
        os.makedirs('output/archive', exist_ok=True)
        output_file = f"output/archive/{TODAY}.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"\nâœ… å·²ä¿å­˜åˆ°: {output_file}")

        # æ˜¾ç¤ºé¢„è§ˆ
        print("\nğŸ“„ å†…å®¹é¢„è§ˆ:")
        print("-" * 60)
        print(content[:500] + "..." if len(content) > 500 else content)
        print("-" * 60)

        return output_file
    else:
        print("\nâŒ æœªèƒ½è·å–åˆ°çœŸå®æ–°é—»ï¼Œä½¿ç”¨AIç”Ÿæˆæ¨¡å¼")
        return None

if __name__ == '__main__':
    main()
