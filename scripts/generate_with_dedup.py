#!/usr/bin/env python3
"""
æ¯æ—¥æ³•å¾‹ç®€æŠ¥ç”Ÿæˆå™¨ - åŒ…å«å†…å®¹å»é‡æœºåˆ¶
1. å…ˆç”Ÿæˆæœ¬åœ°é¢„è§ˆ
2. å¯¹æ¯”å†å²ç®€æŠ¥é¿å…é‡å¤
3. å¦‚æœé‡å¤åˆ™è‡ªåŠ¨é€‰æ‹©å…¶ä»–æ–°é—»
"""

import os
import sys
import json
import glob
import re
from datetime import datetime, timedelta
from difflib import SequenceMatcher

# é…ç½®
HISTORY_DIR = "output/archive"
PREVIEW_DIR = "preview"
TODAY = datetime.now().strftime("%Y-%m-%d")
DISPLAY_DATE = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

def get_history_briefings():
    """è·å–å†å²ç®€æŠ¥åˆ—è¡¨"""
    md_files = sorted(glob.glob(f"{HISTORY_DIR}/*.md"), reverse=True)
    briefings = []

    for md_file in md_files[:7]:  # åªçœ‹æœ€è¿‘7å¤©
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
                # æå–å…³é”®ä¿¡æ¯
                headlines = re.findall(r'### ã€(.+?)ã€‘(.+)', content)
                briefings.append({
                    'file': md_file,
                    'date': os.path.basename(md_file).replace('.md', ''),
                    'headlines': headlines,
                    'content': content
                })
        except:
            continue

    return briefings

def calculate_similarity(text1, text2):
    """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
    return SequenceMatcher(None, text1, text2).ratio()

def check_content_dedup(new_content, history_briefings):
    """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸å†å²é‡å¤"""
    issues = []

    # æå–æ–°å†…å®¹çš„å…³é”®ä¿¡æ¯
    new_headlines = re.findall(r'### ã€(.+?)ã€‘(.+)', new_content)
    new_paragraphs = re.findall(r'\*\*æ‘˜è¦\*\*:(.+?)(?=\n-|$)', new_content, re.DOTALL)

    for hist in history_briefings:
        # æ£€æŸ¥æ ‡é¢˜é‡å¤
        for new_source, new_title in new_headlines:
            for hist_source, hist_title in hist['headlines']:
                if new_title.strip() == hist_title.strip():
                    issues.append({
                        'type': 'title_duplicate',
                        'date': hist['date'],
                        'title': new_title,
                        'source': new_source
                    })

        # æ£€æŸ¥å†…å®¹ç›¸ä¼¼åº¦
        similarity = calculate_similarity(new_content, hist['content'])
        if similarity > 0.7:  # ç›¸ä¼¼åº¦è¶…è¿‡70%
            issues.append({
                'type': 'high_similarity',
                'date': hist['date'],
                'similarity': f"{similarity*100:.1f}%"
            })

    return issues

def fetch_fallback_news():
    """è·å–å¤‡é€‰æ³•å¾‹æ–°é—»ï¼ˆå½“æ£€æµ‹åˆ°é‡å¤æ—¶ä½¿ç”¨ï¼‰"""
    # è¿™é‡Œå¯ä»¥è°ƒç”¨çœŸå®çš„æ–°é—»APIï¼Œæš‚æ—¶ä½¿ç”¨å¤‡ç”¨æ¨¡æ¿
    fallback_news = [
        {
            'source': 'å¸æ³•éƒ¨',
            'title': 'ã€Šå…³äºè¿›ä¸€æ­¥å®Œå–„æ³•å¾‹æ´åŠ©å·¥ä½œçš„å®æ–½æ„è§ã€‹å‘å¸ƒ',
            'time': DISPLAY_DATE,
            'summary': 'å¸æ³•éƒ¨å‘å¸ƒå®æ–½æ„è§ï¼Œè¿›ä¸€æ­¥æ‰©å¤§æ³•å¾‹æ´åŠ©è¦†ç›–é¢ï¼Œæé«˜æ³•å¾‹æ´åŠ©è´¨é‡ã€‚é‡ç‚¹åŠ å¼ºå†œæ°‘å·¥ã€æœªæˆå¹´äººã€æ®‹ç–¾äººç­‰ç‰¹æ®Šç¾¤ä½“çš„æ³•å¾‹æ´åŠ©å·¥ä½œã€‚',
            'impact': 'åˆ‡å®ä¿éšœå›°éš¾ç¾¤ä¼—è·å¾—æ³•å¾‹æ´åŠ©çš„æƒåˆ©ï¼Œä¿ƒè¿›ç¤¾ä¼šå…¬å¹³æ­£ä¹‰ã€‚'
        },
        {
            'source': 'æœ€é«˜äººæ°‘æ³•é™¢',
            'title': 'å‘å¸ƒæœåŠ¡ä¿éšœè‡ªç”±è´¸æ˜“è¯•éªŒåŒºå»ºè®¾å…¸å‹æ¡ˆä¾‹',
            'time': DISPLAY_DATE,
            'summary': 'æœ€é«˜äººæ°‘æ³•é™¢å‘å¸ƒä¸€æ‰¹æœåŠ¡ä¿éšœè‡ªç”±è´¸æ˜“è¯•éªŒåŒºå»ºè®¾çš„å…¸å‹æ¡ˆä¾‹ï¼Œæ¶µç›–å¤–å•†æŠ•èµ„ã€å›½é™…è´¸æ˜“ã€é‡‘èåˆ›æ–°ç­‰é¢†åŸŸï¼Œä¸ºè‡ªè´¸è¯•éªŒåŒºé«˜è´¨é‡å‘å±•æä¾›å¸æ³•ä¿éšœã€‚',
            'impact': 'ä¸ºè‡ªè´¸è¯•éªŒåŒºå»ºè®¾æä¾›æ¸…æ™°çš„å¸æ³•æŒ‡å¼•ï¼Œä¼˜åŒ–è¥å•†ç¯å¢ƒã€‚'
        },
        {
            'source': 'æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢',
            'title': 'éƒ¨ç½²å¼€å±•é£Ÿå“å®‰å…¨ä¸“é¡¹æ£€å¯Ÿç›‘ç£æ´»åŠ¨',
            'time': DISPLAY_DATE,
            'summary': 'æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢å†³å®šåœ¨å…¨å›½èŒƒå›´å†…å¼€å±•é£Ÿå“å®‰å…¨ä¸“é¡¹æ£€å¯Ÿç›‘ç£æ´»åŠ¨ï¼Œé‡ç‚¹æ‰“å‡»å±å®³é£Ÿå“å®‰å…¨çŠ¯ç½ªï¼Œå®Œå–„é£Ÿå“å®‰å…¨é¢†åŸŸæ£€å¯Ÿå…¬ç›Šè¯‰è®¼åˆ¶åº¦ã€‚',
            'impact': 'å®ˆæŠ¤"èˆŒå°–ä¸Šçš„å®‰å…¨"ï¼Œä¿éšœäººæ°‘ç¾¤ä¼—èº«ä½“å¥åº·ã€‚'
        },
        {
            'source': 'ä¸­å›½äººå¤§ç½‘',
            'title': 'ã€Šå¾‹å¸ˆæ³•ã€‹ä¿®è®¢è‰æ¡ˆå…¬å¼€å¾æ±‚æ„è§',
            'time': DISPLAY_DATE,
            'summary': 'å…¨å›½äººå¤§å¸¸å§”ä¼šå…¬å¸ƒã€Šå¾‹å¸ˆæ³•ã€‹ä¿®è®¢è‰æ¡ˆï¼Œå‘ç¤¾ä¼šå…¬å¼€å¾æ±‚æ„è§ã€‚ä¿®è®¢è‰æ¡ˆå®Œå–„äº†å¾‹å¸ˆæ‰§ä¸šæƒåˆ©ä¿éšœæœºåˆ¶ï¼Œè§„èŒƒäº†å¾‹å¸ˆæ‰§ä¸šè¡Œä¸ºï¼ŒåŠ å¼ºäº†å¾‹å¸ˆé˜Ÿä¼å»ºè®¾ã€‚',
            'impact': 'è¿›ä¸€æ­¥å®Œå–„å¾‹å¸ˆåˆ¶åº¦ï¼Œä¿éšœå¾‹å¸ˆä¾æ³•æ‰§ä¸šï¼Œå‘æŒ¥å¾‹å¸ˆåœ¨æ³•æ²»å»ºè®¾ä¸­çš„ä½œç”¨ã€‚'
        }
    ]

    import random
    return random.sample(fallback_news, 3)

def generate_brief_with_dedup():
    """ç”Ÿæˆç®€æŠ¥å¹¶è¿›è¡Œå»é‡æ£€æŸ¥"""

    print(f"ğŸ“… å¼€å§‹ç”Ÿæˆ {DISPLAY_DATE} æ³•å¾‹ç®€æŠ¥")
    print("=" * 60)

    # 1. è¯»å–å†å²ç®€æŠ¥
    print("ğŸ“š æ­£åœ¨åŠ è½½å†å²ç®€æŠ¥...")
    history_briefings = get_history_briefings()
    print(f"   æ‰¾åˆ° {len(history_briefings)} ä»½å†å²ç®€æŠ¥")

    # 2. ç”Ÿæˆåˆå§‹ç®€æŠ¥
    print("ğŸ¤– æ­£åœ¨ç”Ÿæˆç®€æŠ¥å†…å®¹...")
    api_key = os.getenv('GLM_API_KEY')

    if api_key:
        content = generate_with_api(api_key, history_briefings)
    else:
        content = generate_with_template(history_briefings)

    # 3. å»é‡æ£€æŸ¥
    print("ğŸ” æ­£åœ¨æ£€æŸ¥å†…å®¹é‡å¤...")
    issues = check_content_dedup(content, history_briefings)

    if issues:
        print(f"\nâš ï¸  æ£€æµ‹åˆ° {len(issues)} ä¸ªæ½œåœ¨é—®é¢˜:")
        for i, issue in enumerate(issues, 1):
            if issue['type'] == 'title_duplicate':
                print(f"   {i}. æ ‡é¢˜é‡å¤: {issue['title']}")
                print(f"      (ä¸ {issue['date']} é‡å¤)")
            else:
                print(f"   {i}. å†…å®¹ç›¸ä¼¼åº¦è¿‡é«˜: {issue['similarity']}")
                print(f"      (ä¸ {issue['date']} ç›¸ä¼¼)")

        print("\nğŸ”„ æ­£åœ¨ç”Ÿæˆå¤‡é€‰å†…å®¹...")
        fallback_news = fetch_fallback_news()

        # é‡æ–°æ„å»ºç®€æŠ¥ï¼ˆä½¿ç”¨å¤‡é€‰å†…å®¹ï¼‰
        content = build_briefing_content(fallback_news)
        print("âœ… å·²ä½¿ç”¨å¤‡é€‰å†…å®¹é‡æ–°ç”Ÿæˆç®€æŠ¥\n")
    else:
        print("âœ… å†…å®¹æ£€æŸ¥é€šè¿‡ï¼Œæ— é‡å¤é—®é¢˜\n")

    # 4. ä¿å­˜é¢„è§ˆ
    os.makedirs(PREVIEW_DIR, exist_ok=True)
    preview_file = f"{PREVIEW_DIR}/{TODAY}.md"

    with open(preview_file, 'w', encoding='utf-8') as f:
        f.write(content)

    print(f"ğŸ“ æœ¬åœ°é¢„è§ˆå·²ä¿å­˜: {preview_file}")
    print("=" * 60)

    # 5. æ˜¾ç¤ºé¢„è§ˆ
    print("\nğŸ“„ ç®€æŠ¥é¢„è§ˆ:")
    print("-" * 60)
    print(content[:500] + "..." if len(content) > 500 else content)
    print("-" * 60)

    # 6. è¯¢é—®ç¡®è®¤
    print(f"\nâ“ æ˜¯å¦ç¡®è®¤å‘å¸ƒåˆ°æ­£å¼ç›®å½•? ({HISTORY_DIR}/{TODAY}.md)")
    print("   [y] æ˜¯ï¼Œå‘å¸ƒ")
    print("   [n] å¦ï¼Œå–æ¶ˆ")
    print("   [e] ç¼–è¾‘åé‡æ–°ç”Ÿæˆ")

    choice = input("\nè¯·é€‰æ‹© [y/n/e]: ").strip().lower()

    if choice == 'y':
        # å‘å¸ƒåˆ°æ­£å¼ç›®å½•
        os.makedirs(HISTORY_DIR, exist_ok=True)
        output_file = f"{HISTORY_DIR}/{TODAY}.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"\nâœ… ç®€æŠ¥å·²å‘å¸ƒ: {output_file}")
        return output_file

    elif choice == 'e':
        # æ‰“å¼€ç¼–è¾‘å™¨
        import subprocess
        editor = os.getenv('EDITOR', 'vim')
        subprocess.call([editor, preview_file])

        # é‡æ–°è¯»å–å¹¶ä¿å­˜
        with open(preview_file, 'r', encoding='utf-8') as f:
            edited_content = f.read()

        output_file = f"{HISTORY_DIR}/{TODAY}.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(edited_content)

        print(f"\nâœ… ç¼–è¾‘åçš„ç®€æŠ¥å·²å‘å¸ƒ: {output_file}")
        return output_file

    else:
        print("\nâŒ å·²å–æ¶ˆå‘å¸ƒ")
        return None

def generate_with_api(api_key, history_briefings):
    """ä½¿ç”¨GLM APIç”Ÿæˆç®€æŠ¥"""
    import requests

    # è·å–å†å²æ ‡é¢˜ç”¨äºå»é‡æç¤º
    history_titles = set()
    for hist in history_briefings:
        for source, title in hist['headlines']:
            history_titles.add(title.strip())

    exclude_hint = ""
    if history_titles:
        exclude_hint = f"\n\næ³¨æ„ï¼šè¯·é¿å…ä½¿ç”¨ä»¥ä¸‹å·²å‡ºç°çš„æ ‡é¢˜ï¼š\n" + "\n".join(list(history_titles)[:5])

    prompt = f"""è¯·ç”Ÿæˆä¸€ä»½{DISPLAY_DATE}çš„æ³•å¾‹ç®€æŠ¥ã€‚

è¦æ±‚ï¼š
1. åŒ…å«3-5æ¡ä»Šæ—¥æ³•å¾‹è¦é—»ï¼ˆå¿…é¡»æ˜¯{DISPLAY_DATE}çš„æœ€æ–°æ–°é—»ï¼‰
2. æ¶µç›–æœ€é«˜æ³•ã€æœ€é«˜æ£€ã€å¸æ³•éƒ¨ç­‰å®˜æ–¹åŠ¨æ€
3. ä¸“ä¸šç®€æ´çš„æ‘˜è¦ï¼Œæ¯æ¡æ–°é—»è¦æœ‰ç‹¬ç‰¹æ€§å’Œæ—¶æ•ˆæ€§
4. Markdownæ ¼å¼è¾“å‡º{exclude_hint}

è¾“å‡ºæ ¼å¼ï¼š
# {DISPLAY_DATE} æ³•å¾‹ç®€æŠ¥

**å¯¼è¯­ï¼š** ç®€çŸ­å¯¼è¯­ï¼ˆ2-3å¥è¯æ€»ç»“å½“æ—¥æ³•å¾‹åŠ¨æ€ï¼‰

---

## 1. ä»Šæ—¥è¦é—»

### ã€æ¥æºã€‘æ ‡é¢˜
- **æ¥æº**: xxx
- **æ—¶é—´**: {DISPLAY_DATE}
- **æ‘˜è¦**: xxx
- **å®åŠ¡å½±å“**: xxx

---

*æœ¬ç®€æŠ¥ç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å­¦ä¹ å‚è€ƒ*
"""

    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        data = {
            "model": "glm-4.7",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.8,  # æé«˜æ¸©åº¦ä»¥å¢åŠ å¤šæ ·æ€§
            "max_tokens": 2000
        }

        response = requests.post(
            "https://open.bigmodel.cn/api/paas/v4/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )

        if response.status_code == 200:
            result = response.json()
            return result['choices'][0]['message']['content']
        else:
            print(f"âš ï¸  GLM APIè°ƒç”¨å¤±è´¥: {response.status_code}")
            return generate_with_template(history_briefings)

    except Exception as e:
        print(f"âš ï¸  GLM APIè°ƒç”¨å¼‚å¸¸: {e}")
        return generate_with_template(history_briefings)

def generate_with_template(history_briefings):
    """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆç®€æŠ¥ï¼ˆåŒ…å«å»é‡é€»è¾‘ï¼‰"""

    # ä½¿ç”¨å¤‡é€‰æ–°é—»
    fallback_news = fetch_fallback_news()
    return build_briefing_content(fallback_news)

def build_briefing_content(news_items):
    """æ„å»ºç®€æŠ¥å†…å®¹"""
    content = f"""# {DISPLAY_DATE} æ³•å¾‹ç®€æŠ¥

**å¯¼è¯­ï¼š** ä»Šæ—¥æ³•å¾‹ç•Œæœ€æ–°èµ„è®¯æ›´æ–°ã€‚

---

## 1. ä»Šæ—¥è¦é—»

"""

    for news in news_items:
        content += f"""### ã€{news['source']}ã€‘{news['title']}

- **æ¥æº**: {news['source']}
- **æ—¶é—´**: {news['time']}
- **æ‘˜è¦**: {news['summary']}
- **å®åŠ¡å½±å“**: {news['impact']}

"""

    content += """
---

*æœ¬ç®€æŠ¥ç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å­¦ä¹ å‚è€ƒï¼Œä¸æ„æˆæ³•å¾‹å»ºè®®*
"""

    return content

if __name__ == '__main__':
    generate_brief_with_dedup()
